{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markow Text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution by: Akshay Punwatkar(AP509) and Ishan Gupta (IG55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from collections import defaultdict \n",
    "from pprint import pprint\n",
    "import operator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_sentence(sentence, n, corpus, deterministic=False):\n",
    "    \"\"\"\n",
    "    Input:-\n",
    "    sentence     : A sentence [list of tokens] that we’re trying to build on\n",
    "    n [int]      : The length of n-grams to use for prediction, and\n",
    "    corpus [list]: Source corpus [list of tokens]\n",
    "    deterministic: Flag indicating whether the process should be deterministic [bool]\n",
    "    \n",
    "    If deterministic is true ; Choose at each step the single most probable next token. \n",
    "                               When two tokens are equally probable, choose the lesser one (according to Python).\n",
    "    If deterministic is false; Draw the next word randomly from the appropriate distribution. Use stupid backoff and no smoothing.\n",
    "    \n",
    "    Output:-\n",
    "    Returns an extended sentence until the first ., ?, or ! is found OR until it has 15 total tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    if n < 2:\n",
    "        return \"Error: n should be greater than 1 to make predictions\"\n",
    "    \n",
    "    #removing non-required character and numbers from corpus\n",
    "    corpus[:] = [item.strip('\"') for item in corpus if not (item in ['[',']',\"*\",\"-\",\";\",\":\",\".--\",\"--\",\",\",\";--\"] or item.isdigit())]\n",
    "#     corpus = corpus[:50]\n",
    "    n_gram_array_list = []\n",
    "\n",
    "    # an n-gram model looks n-1 words into the past (eg. a trigram (3-gram) model looks two words into the past)\n",
    "    start_time = time.time()\n",
    "    # creating n-gram (for n)\n",
    "    for j in range(0,n-1):\n",
    "        n_gram_size = j+2\n",
    "        n_gram_array = []\n",
    "        for i in range(len(corpus)-n_gram_size+1):\n",
    "            n_gram_array.append(corpus[i:(i+n_gram_size)])\n",
    "        n_gram_array = np.array(n_gram_array)    \n",
    "        n_gram_array_list.append(n_gram_array)    \n",
    "#     print(\"Time taken for array : %s\"%round(time.time()-start_time,3))\n",
    "    \n",
    "    #converting to numpy-array\n",
    "    n_gram_array_list = np.array(n_gram_array_list,dtype=object)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #creating n-gram dictionary\n",
    "    #n_gram_dict = defaultdict(lambda: defaultdict(int)) \n",
    "    n_gram_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) \n",
    "    \n",
    "    for j in range(0,n-1):\n",
    "        for n_gram in n_gram_array_list[j]:\n",
    "            n_gram_dict[j+2][tuple(n_gram[:-1])][n_gram[-1]] += 1\n",
    "#     print(\"Time taken for Dictionary creation : %s\"%round(time.time()-start_time,3))\n",
    "    #auto completing a sentence\n",
    "    \n",
    "    start_time = time.time()\n",
    "    w_in_sentc = [w.lower() for w in sentence]\n",
    "    for i in range(15):\n",
    "        suggest_word = w_in_sentc[-1]\n",
    "        \n",
    "        for i in range(n,1,-1):\n",
    "            n_gram_key = tuple(w_in_sentc[(-i+1):])\n",
    "            \n",
    "            if n_gram_key in n_gram_dict[i].keys():\n",
    "                if deterministic == True:\n",
    "                    most_probab = max(n_gram_dict[i][n_gram_key].values())\n",
    "                    opts = []\n",
    "                    for k,v in n_gram_dict[i][n_gram_key].items():\n",
    "                        if v == most_probab:\n",
    "                            opts.append(k) \n",
    "                    suggest_word = np.sort(opts)[0]\n",
    "                    break\n",
    "                else:\n",
    "                    suggest_word = np.random.choice(list(n_gram_dict[i][n_gram_key].keys()))\n",
    "                    break\n",
    "        \n",
    "        w_in_sentc.append(suggest_word)  \n",
    "        \n",
    "        if suggest_word in [\".\",\"?\",\"!\"]:\n",
    "#             print(\"Time taken for search : %s\"%round(time.time()-start_time,3))\n",
    "            return w_in_sentc\n",
    "#     print(\"Time taken for search : %s\"%round(time.time()-start_time,3))    \n",
    "    return w_in_sentc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case\n",
    "sentence = ['she', 'was', 'not'] \n",
    "n = 3\n",
    "corpus = [w.lower() for w in nltk.corpus.gutenberg.words('austen-sense.txt')]\n",
    "deterministic = True\n",
    "\n",
    "#expected output:\n",
    "#[’she’, ’was’, ’not’, ’in’, ’the’, ’world’, ’.’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'was', 'not', 'in', 'the', 'world', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish_sentence(sentence, n, corpus, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akshay', 'was', 'not', 'in', 'the', 'world', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish_sentence([\"akshay\",\"was\"], 3, corpus, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akshay',\n",
       " 'was',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'very',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she',\n",
       " 'was',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'very',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish_sentence([\"akshay\",\"was\"], 2, corpus, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akshay',\n",
       " 'was',\n",
       " 'consoled',\n",
       " 'for',\n",
       " 'every',\n",
       " 'past',\n",
       " 'affliction',\n",
       " 'her',\n",
       " 'regard',\n",
       " 'without',\n",
       " 'a',\n",
       " 'thought',\n",
       " 'struck',\n",
       " 'him',\n",
       " 'when',\n",
       " 'edward',\n",
       " 'did']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish_sentence([\"akshay\",\"was\"], 3, corpus, deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
