{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markow Text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution by: Akshay Punwatkar(AP509) and Ishan Gupta (IG55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from collections import defaultdict \n",
    "from pprint import pprint\n",
    "import operator\n",
    "import time\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoComplete():\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.n = 0\n",
    "        self.corpus_hash = 0\n",
    "        \n",
    "    def clean_corpus(self):\n",
    "        self.corpus[:] = [item.strip('\"') for item in self.corpus if not \n",
    "                     (item in ['[',']',\"*\",\"-\",\";\",\":\",\".--\",\"--\",\",\",\";--\"] or item.isdigit())]\n",
    "        \n",
    "    def create_n_gram_array(self, n):\n",
    "        n_gram_size = n\n",
    "        n_gram_array = []\n",
    "        for i in range(len(self.corpus)-n_gram_size+1):\n",
    "            n_gram_array.append(corpus[i:(i+n_gram_size)])\n",
    "        n_gram_array = np.array(n_gram_array)\n",
    "        self.n_gram_array_list.append(n_gram_array)\n",
    "        return self.n_gram_array_list\n",
    "    \n",
    "    def create_n_gram_dict(self, n):\n",
    "        for n_gram in self.n_gram_array_list[n-2]:\n",
    "            self.n_gram_dict[n][tuple(n_gram[:-1])][n_gram[-1]] += 1\n",
    "    \n",
    "    def create_n_gram(self,corpus,n):\n",
    "        self.corpus_hash = hashlib.sha224(\" \".join(corpus).encode('utf-8')).hexdigest()\n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        self.n_gram_array_list = []\n",
    "        self.n_gram_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) \n",
    "        self.clean_corpus()\n",
    "        for i in range(2,n+1):\n",
    "            self.create_n_gram_array(i)\n",
    "            self.create_n_gram_dict(i)\n",
    "            \n",
    "    def finish_sentence(self, sentence, n, corpus, deterministic=False):\n",
    "        \"\"\"\n",
    "        Input:-\n",
    "            sentence     : A sentence [list of tokens] that we’re trying to build on\n",
    "            n [int]      : The length of n-grams to use for prediction, and\n",
    "            corpus [list]: Source corpus [list of tokens]\n",
    "            deterministic: Flag indicating whether the process should be deterministic [bool]\n",
    "    \n",
    "        If deterministic is true ; Choose at each step the single most probable next token. \n",
    "                               When two tokens are equally probable, choose the lesser one (according to Python).\n",
    "        If deterministic is false; Draw the next word randomly from the appropriate distribution. Use stupid backoff and no smoothing.\n",
    "    \n",
    "        Output:-\n",
    "            Returns an extended sentence until the first ., ?, or ! is found OR until it has 15 total tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.n < n or self.corpus_hash != hashlib.sha224(\" \".join(corpus).encode('utf-8')).hexdigest():\n",
    "            self.create_n_gram(corpus, n)\n",
    "            \n",
    "        self.w_in_sentc = [w.lower() for w in sentence]\n",
    "        for i in range(15):\n",
    "            suggest_word = self.w_in_sentc[-1]\n",
    "            for i in range(n,1,-1):\n",
    "                n_gram_key = tuple(self.w_in_sentc[(-i+1):])\n",
    "\n",
    "                if n_gram_key in self.n_gram_dict[i].keys():\n",
    "                    if deterministic == True:\n",
    "                        most_probab = max(self.n_gram_dict[i][n_gram_key].values())\n",
    "                        opts = []\n",
    "                        for k,v in self.n_gram_dict[i][n_gram_key].items():\n",
    "                            if v == most_probab:\n",
    "                                opts.append(k) \n",
    "                        suggest_word = np.sort(opts)[0]\n",
    "                        break\n",
    "                    else:\n",
    "                        suggest_word = np.random.choice(list(self.n_gram_dict[i][n_gram_key].keys()))\n",
    "                        break\n",
    "            self.w_in_sentc.append(suggest_word)  \n",
    "            if suggest_word in [\".\",\"?\",\"!\"]:\n",
    "                return self.w_in_sentc\n",
    "            \n",
    "        return self.w_in_sentc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case\n",
    "sentence = ['she', 'was', 'not'] \n",
    "n = 3\n",
    "corpus = [w.lower() for w in nltk.corpus.gutenberg.words('austen-sense.txt')]\n",
    "deterministic = True\n",
    "ac = autoComplete()\n",
    "#expected output:\n",
    "#[’she’, ’was’, ’not’, ’in’, ’the’, ’world’, ’.’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'was', 'not', 'in', 'the', 'world', '.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.finish_sentence(sentence, n, corpus, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akshay', 'was', 'not', 'in', 'the', 'world', '.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.finish_sentence([\"akshay\",\"was\"], 3, corpus, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akshay',\n",
       " 'was',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'very',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she',\n",
       " 'was',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'very',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.finish_sentence([\"akshay\",\"was\"], 2, corpus, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akshay',\n",
       " 'was',\n",
       " 'very',\n",
       " 'unwilling',\n",
       " 'to',\n",
       " 'run',\n",
       " 'the',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'being',\n",
       " 'too',\n",
       " 'weak',\n",
       " 'for',\n",
       " 'conversation',\n",
       " 'submitted',\n",
       " 'readily',\n",
       " 'to']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.finish_sentence([\"akshay\",\"was\"], 3, corpus, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'was', 'not', 'in', 'the', 'world', '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.finish_sentence(sentence, 3, corpus, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
